---
title: Firearm Legislation and Fatal Police Shootings in the US
output: 
    html_document:
        theme: cosmo 
        toc: true
        toc_float: true
        highlight: tango
        number_sections: false
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

# Motivation
Here we will recreate a study completed by the American Journal of Public Health "to examine whether stricter firearm legislation is associated with rates of fatal police shootings." https://ajph.aphapublications.org/doi/suppl/10.2105/AJPH.2017.303770

# What is the data?
This study uses data from several different sources.

1. The Brady Campain State Scorecard from 2015. This provides numerical scores for each state across different types of firearm legislation. Obtained at http://crimadvisor.com/data/Brady-State-Scorecard-2015.pdf.
2. The Counted: People killed by police in the US. Because "the US government has no comprehensive record of the number of people killed by law enforcement," The Counted project started and is known as a more accurate source for this data. https://www.theguardian.com/us-news/ng-interactive/2015/jun/01/about-the-counted.
3. Sociodemographic data, such as age, race, gender, and total population by state obtained from the US Census.https://www.census.gov/content/census/en/data/tables/2017/demo/popest/state-detail.html.
4. Violent crime data in 2015 by state.Obtained from the FBI's Uniform crime report https://ucr.fbi.gov/crime-in-the-u.s/2015/crime-in-the-u.s.-2015/tables/table-5.
5. Unemployment rate data: https://www.bls.gov/lau/lastrk15.htm
6. To calculate population density in 2015, we must import land area data from the US census (2010 land area data). https://www.census.gov/support/USACdataDownloads.html#LND
7. Need education data

# Data import
```{r}
library(readxl)
#census data
census <- read.csv("sc-est2017-alldata6.csv",nrows = 236900,stringsAsFactors = FALSE)
head(census[, 1:5])
# Brady scores
brady <- read_excel("Brady-State-Scorecard-2015.xlsx", sheet = 1)
head(brady[, 1:5])
# The Counted data
counted15 <- read.csv("the-counted-2015.csv",stringsAsFactors = FALSE)
counted16 <- read.csv("the-counted-2016.csv",stringsAsFactors = FALSE)
head(counted15)
# Crime data
crime <- read_excel("table_5_crime_in_the_united_states_by_state_2015.xls", sheet = 1,skip = 3)
head(crime[, 1:5])
# Land area data
land <- read_excel("LND01.xls", sheet = 1)
head(land[, 1:5])
```


# Data wrangling
## 1. Census Data
We need to find "age; percentage of study population that was male, White, Black, Hispanic, ... at the state level" using the census data.

The following is taken from the document sc-est2017-alldata6.pdf:

  * The key for SEX is as follows:
    + 0 = Total
    + 1 = Male
    + 2 = Female

  * The key for ORIGIN is as follows: 
    + 0 = Total
    + 1 = Not Hispanic
    + 2 = Hispanic

  * The key for RACE is as follows:
    + 1 = White Alone
    + 2 = Black or African American Alone
    + 3 = American Indian and Alaska Native Alone
    + 4 = Asian Alone
    + 5 = Native Hawaiian and Other Pacific Islander Alone 6 = Two or more races

So we need to group the data in several different ways and calculate the necessary statistics.
Let's start with "percentage of the study population that was male." For each state, we need to add every row of data that is "male" (SEX = 1) in the column POPESTIMATE2015 since we are looking at the year 2015.
```{r}
library(dplyr)
maleStats <- census %>%
  filter(ORIGIN == 0) %>% #set origin to "Total" so we don't have repeats in the data.
  group_by(NAME) %>% #group by the state
  summarize(percentMale = sum(POPESTIMATE2015[SEX==1])/sum(POPESTIMATE2015[SEX==0])*100) #divide total male by total  population
head(maleStats)
```
Next let's tackle age. We need to find the median age for each state. This is tricky, since we don't have a list of ages (i.e., (22,55,4,27,35)) but instead we have the counts of the age. First we need to get the total counts for each age (per state), since that's divided up into all the other categories.
```{r}
library(dplyr)
library(tidyr)
library(data.table)
ageStats <- census %>%
  filter(ORIGIN == 0,SEX == 0) %>% #set origin and sex to "Total" so we don't have repeats in the data.
  group_by(NAME,AGE) %>% #group by state and age
  summarize(sumAges = sum(POPESTIMATE2015)) #divide total male by total  population
head(ageStats)
# transform matrix to have state names as column heaeders
ageStats <- dcast(setDT(ageStats), AGE ~ NAME, value.var="sumAges")
ageStats$AGE <- NULL #get rid of the age columns since we can just use the index minus 1
ageStats <- apply(ageStats, 2, cumsum) # take the cumulative sum of each column
ageStats <- as.data.frame(ageStats) # convert back to a dataframe
ageStats <- apply(ageStats,2,function(x) x/x[86]) # divide each column by the last value, showing the cumulative sum at that age as a percentile
ageStats <- as.data.frame(ageStats) # convert back to a dataframe
# now we have a percentile associated with each age
# now we just need to get the age of the 50th percentile for each state...
head(ageStats[, 1:6])
median <- apply(ageStats,2,function(x) which.max(x >= 0.5)-1) 
medians <- as.data.frame(median)
head(medians)

```
Find the proportion of White, Black, and Hispanic residents per state.
```{r}
library(dplyr)
raceStats <- census %>%
  filter(ORIGIN == 0, SEX == 0) %>% #set origin and sex to "Total" so we don't have repeats in the data.
  group_by(NAME) %>% #group by the state
  summarize(percentWhite = sum(POPESTIMATE2015[RACE==1])/sum(POPESTIMATE2015)*100,
            percentBlack = sum(POPESTIMATE2015[RACE==2])/sum(POPESTIMATE2015)*100) 
head(raceStats)

originStats <- census %>%
  filter(SEX == 0) %>% #set origin and sex to "Total" so we don't have repeats in the data.
  group_by(NAME) %>% #group by the state
  summarize(percentHispanic = sum(POPESTIMATE2015[ORIGIN==2])/sum(POPESTIMATE2015[ORIGIN==0])*100)
head(originStats)
```
## 2. Violent Crime
```{r}
library(dplyr)
library(zoo)
head(crime)
# Selecting "Violent crime1" was not working, so I printed "colnames(crime)" to see that there are some extraneous characters in the names. Instead I take the index from the column names. I also could probably just select the index of the columns.
violentcrime<- crime %>%
  select("State",colnames(crime)[3], colnames(crime)[5]) 
violentcrime[10:20,] 
#violentcrime$State <- na.locf(violentcrime$State)
#(violentcrime)
violentcrime2<- violentcrime %>%
  group_by(State) %>% 
  na.locf %>%
  ungroup
violentcrime2[10:20,] 
# Hmm..I don't want to fill na for the other columns just yet because now there are duplicate rows with different states, making it confusing.
# Let's do a quick fix
violentcrime$State <- violentcrime2$State
violentcrime[10:20,]
# I really want the "Rate per 100,000 inhabitants" for violent crime for each state.
violentcrime<- subset(violentcrime, X__1 == "Rate per 100,000 inhabitants")
violentcrime$X__1 <- NULL
names(violentcrime) <- c("State", "Violent_Crime_rate_per_100,000")
head(violentcrime)
# Looking at our dataframe we can see some of the state names have numbers in them. This will make it hard to later merge this data together, so we should clean this up.
violentcrime$State<-gsub('[0-9]+', '', violentcrime$State)
```

## 3. Brady Scores
The study by AJPH groups the scores by the 7 categories. It is also important to note that the study removed all weightings of the different laws in favor of a "1 law 1 point" system, since the weightings were "somewhat arbitrary."
```{r}
#install.packages("rJava")
#install.packages(c("NLP", "openNLP", "RWeka", "qdap"))
require(rJava)
require(openNLP)
library(rJava)
library(NLP)
library(openNLP)
#library(RWeka)

PHlaws <- readLines("supplemental table a. .txt")
blaws <- brady[1]

```



## 4. The Counted Fatal Shootings
```{r}
# first combine the two years of data
# install.packages("openintro")
library(openintro)
counted1516 <- rbind(counted15, counted16)
counted1516$state <-toupper(abbr2state(counted1516$state))
head(counted1516)
```
## 5. Unemployment Data
This data is available online, but there is no easy download of the table. It is also hard to simply copy and paste; it doesn't hold it's table format. Thus, let's set up a web scraper to get the data.

```{r}
# install.packages("rvest")
library(rvest)
url <- read_html("https://www.bls.gov/lau/lastrk15.htm")
datavalues <- url%>%
  html_nodes('.datavalue') %>% # html_nodes acts as a CSS selector
  html_text() %>%
  as.numeric()

state <- url%>%
  html_nodes('.sub0') %>%
  html_text()

unemployment<-as.data.frame(state[c(2:52)]) # the first listing is united states and we don't need that
# since "datavalue" was the class name for both fields, we need to seperate them by taking every other entry as rank and every other as unemployment rate
# skip the first two entries
rate_select <- seq(3,103,by=2)
rank_select <- seq(4,104,by=2)
unemployment$rate <- datavalues[rate_select]
unemployment$rank <- datavalues[rank_select]
head(unemployment)
```
## 6. Population Density 2015 
We can calculate total 2015 population from the census data and import land area to calculate population density.

```{r}
library(dplyr)
totalPop <- census %>%
  filter(ORIGIN == 0, SEX == 0 ) %>% #set origin & sex to "Total" so we don't have repeats in the data.
  group_by(NAME) %>% #group by the state
  summarize(total = sum(POPESTIMATE2015)) #divide total male by total 
landSqMi <- land %>%
  select(Areaname,LND110210D) # I selected LND110210D because I looked at the data table at https://www.census.gov/geo/reference/state-area.html and compared values to find the correct column. This one corresponds to land area in square miles

# Now we just need to do an inner merge on the state and areaname so that we just get the area for each state.
names(landSqMi) <- c("NAME", "LandAreaSqMiles")
landSqMi$NAME <- toupper(landSqMi$NAME) # need to make both columns 
totalPop$NAME <- toupper(totalPop$NAME)
popdensity <- merge(totalPop,landSqMi,by="NAME")
popdensity$density <- popdensity$total/popdensity$LandAreaSqMiles
head(popdensity)
```
# Exploratory data analysis
```{r}
# "We calculated descriptive statistics for the proportion of victims that were male, armed, and non-White."
fatal_gunshot <- counted1516 %>%
  filter(classification == "Gunshot",raceethnicity != "white", armed != "No", gender == "Male") %>%
  group_by(state) %>% #group by the state
  tally() # just count the number of observations in each of the grouped categories
head(fatal_gunshot)
```

# Data analysis 

# Summary of results


