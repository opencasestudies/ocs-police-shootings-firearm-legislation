---
title: "Firearm Legislation and Fatal Police Shootings in the US"
author: "Alexandra Stephens"
output:
  html_document:
    highlight: tango
    number_sections: no
    theme: cosmo
    toc: yes
    toc_float: yes
  pdf_document:
    toc: yes
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```
# Motivation
We will recreate a study by the American Journal of Public Health "to examine whether stricter firearm legislation is associated with rates of fatal police shootings" (https://ajph.aphapublications.org/doi/suppl/10.2105/AJPH.2017.303770).

The bulk of this code will consist of collecting, cleaning and manipulating data in order to control for potential covariates. We will also run a poisson regression and tests of statistical significance.

The packages used in this study are dplyr, rvest, ggplot2, zoo, openintro, readxl, data.table, and sandwich. If you are running this code please insure you have these packages installed.

The learning objectives include data cleaning and manipulation, problem solving, scraping data from the web, cleaning text, merging data frames, poisson regression and significance testing.

# What is the data?
This study uses data from many different sources.

1. The Brady Campaign State Scorecard 2015: numerical scores for firearm legislation in each state. Obtained at http://crimadvisor.com/data/Brady-State-Scorecard-2015.pdf.
2. The Counted: Persons killed by police in the US. The Counted project started because "the US government has no comprehensive record of the number of people killed by law enforcement." Visit https://www.theguardian.com/us-news/ng-interactive/2015/jun/01/about-the-counted.
3. US Census: Population characteristics by state. https://www.census.gov/content/census/en/data/tables/2017/demo/popest/state-detail.html.
4. FBI's Uniform Crime Report: https://ucr.fbi.gov/crime-in-the-u.s/2015/crime-in-the-u.s.-2015/tables/table-5.
5. Unemployment rate data: https://www.bls.gov/lau/lastrk15.htm
6. US Census 2010 Land Area: https://www.census.gov/support/USACdataDownloads.html#LND
7. We need education data for 2015. (Continue analysis without this for now).

# Data import
We import the Census population characteristics data, Brady Campaign Scorecard (2015), The Counted data for the years of 2015 and 2016, the FBI's crime report for 2015, and land area by state from the Census. These files were downloaded from their respective websites listed in the above section "What is the data?"
```{r}
library(readxl)
census <- read.csv("sc-est2017-alldata6.csv",nrows = 236900,stringsAsFactors = FALSE)
head(census[, 1:5])
brady <- read_excel("Brady-State-Scorecard-2015.xlsx", sheet = 1)
head(brady[, 1:5])
counted15 <- read.csv("the-counted-2015.csv",stringsAsFactors = FALSE)
counted16 <- read.csv("the-counted-2016.csv",stringsAsFactors = FALSE)
head(counted15)
crime <- read_excel("table_5_crime_in_the_united_states_by_state_2015.xls", sheet = 1,skip = 3)
head(crime[, 1:5])
land <- read_excel("LND01.xls", sheet = 1)
head(land[, 1:5])
```


# Data wrangling
## 1. Census Data
### Percent Male, White, Black, Hispanic

The following is taken from the document sc-est2017-alldata6.pdf:

  * The key for SEX is as follows:
    + 0 = Total
    + 1 = Male
    + 2 = Female

  * The key for ORIGIN is as follows: 
    + 0 = Total
    + 1 = Not Hispanic
    + 2 = Hispanic

  * The key for RACE is as follows:
    + 1 = White Alone
    + 2 = Black or African American Alone
    + 3 = American Indian and Alaska Native Alone
    + 4 = Asian Alone
    + 5 = Native Hawaiian and Other Pacific Islander Alone
    + 6 = Two or more races

Here we use the dplyr package to filter, group, and summarize the data in order to calculate the necessary statistics.
For each state, we add rows in the column POPESTIMATE2015 since we are looking at the year 2015.
Setting the ORIGIN or SEX equal to 0 ensures we don't add duplicate data, since 0 is the key for both Hispanic and non Hispanic residents and total male and female residents.
We group by each state since all data in this study is at the state level.
```{r}
library(dplyr)
census_stats <- census %>%
  filter(ORIGIN == 0, SEX == 0) %>%
  group_by(NAME) %>%
  summarize(white = sum(POPESTIMATE2015[RACE==1])/sum(POPESTIMATE2015)*100,
            black = sum(POPESTIMATE2015[RACE==2])/sum(POPESTIMATE2015)*100)

census_stats$hispanic <- census %>%
  filter(SEX == 0) %>% 
  group_by(NAME) %>%
  summarize(x = sum(POPESTIMATE2015[ORIGIN==2])/sum(POPESTIMATE2015[ORIGIN==0])*100) %>%
  pull(x)

census_stats$male <- census %>%
  filter(ORIGIN == 0) %>%
  group_by(NAME) %>%
  summarize(x = sum(POPESTIMATE2015[SEX==1])/sum(POPESTIMATE2015[SEX==0])*100) %>%
  pull(x)

census_stats$total_pop <- census %>%
  filter(ORIGIN == 0, SEX == 0 ) %>%
  group_by(NAME) %>%
  summarize(total = sum(POPESTIMATE2015)) %>%
  pull(total)

head(census_stats)
```
### Median Age
First we need to get the total counts for each age (per state), since that's divided up into all the other categories.
Set origin and sex to "Total" so we don't have repeats in the data.
Group by state and age since these are the two factors we need to keep seperated.
The column sum_ages is the total residents in each specified state at each age year.
```{r}
age_stats <- census %>%
  filter(ORIGIN == 0,SEX == 0) %>%
  group_by(NAME,AGE) %>%
  summarize(sum_ages = sum(POPESTIMATE2015))
head(age_stats)
```
We begin to see that finding the median is tricky, since we don't have a nice list of ages (i.e., [22,55,4,27,...,35]) for each state, in which case we could simply use a built in R "median" function. Instead we have the number of people that are ages 0-85+ in each state. For example, the first row of data above says that in Alabama (in 2015), there are 59080 residents that are 0 years old.

First let us transform the matrix to have each state as a column name with the sumAges data underneath. I.e., right now we have a 51*86 by 3 dataframe, and instaed we want a 51 by 86 dataframe. I removed the age column since we can use the index instead (index minus 1, since R dataframes are indexed from 1 and the ages start at 0).
```{r}
library(data.table)
age_stats <- dcast(setDT(age_stats), AGE ~ NAME, value.var="sum_ages")
age_stats$AGE <- NULL
head(age_stats[,0:5])
```
Now that we've made the data easier to work with, we need to find a way to get the median. One method is to take the cumulative sum of each column and then divide all the rows by the last row in each repective column, calculating a percentile/quantile for each age. Then, we need to find the age in each state where the population exceeds the 50th percentile (the median!) for the first time.
```{r}
age_stats <- as.data.frame(apply(age_stats, 2, cumsum))
age_stats <- as.data.frame(apply(age_stats,2,function(x) x/x[86]))
head(age_stats[, 1:6])
census_stats$age <- apply(age_stats,2,function(x) which.max(x >= 0.5)-1)
head(census_stats)
```
## 2. Violent Crime
First print the names of the columns to see that there are some extraneous characters not visible in the dataframe. Use the column index to select columns instead of the complicated names.
Also, we print a specified row of violent crime to observe the X__1 group we are looking for -- "Rate per 100,000 inhabitants" (per the study.)
```{r}
#library(dplyr)
colnames(crime)
violentcrime<- crime[ , c(1,3,5)]
violentcrime[11, ]
```
We need get all rows where X__1 = Rate per 100,000 inhabitants. However, as we can see above, the value for "State" in these rows is NA. We need to fill that value with the state name that is listed in a previous row.
Then we can select the rows where X__1 = Rate per 100,000 inhabitants.
After that, we no longer need the column X__1, so we can remove it.
```{r}
library(zoo)
violentcrime$State <- na.locf(violentcrime$State)
violentcrime[11, ]
violentcrime<- subset(violentcrime, X__1 == "Rate per 100,000 inhabitants")
violentcrime$X__1 <- NULL
names(violentcrime) <- c("State", "violent_crime")
head(violentcrime)

```
Looking at our whole dataframe we can see some of the state names have numbers in them (an example is printed below). This will make it hard to later merge this data together, so we should clean this up. We also make the state names all lowercase for ease of merging.
```{r}
violentcrime$State[20]
violentcrime$State<-tolower(gsub('[0-9]+', '', violentcrime$State))
violentcrime$State[20]
```

## 3. Brady Scores
The study by AJPH groups the scores by the 7 categories. It is also important to note that the study removed all weightings of the different laws in favor of a "1 law 1 point" system, since the weightings were "somewhat arbitrary."

For the purpose of practice and simplification (for now), let's just keep the first line of "total state points" from the Brady Scorecard.

We need to transform the data frame so that we have a column of state names and a column of the corresponding total scores.

The scores are characters from the data import, so we change them to numeric.
```{r}
head(brady)
colnames(brady)[1] <- "Law"
brady <- brady %>%
  filter(Law == "TOTAL STATE POINTS")
head(brady)
```
```{r}
brady <- brady[,(ncol(brady) - 49):ncol(brady)]
```

```{r}
brady_totals <- as_data_frame(colnames(brady))
brady_totals$brady_scores <- t(brady)
colnames(brady_totals)[1] <- "state"
brady_totals$brady_scores <- as.numeric(brady_totals$brady_scores)
head(brady_totals)
```

In addition, this data frame's list of states is in two-letter state abbreviations. Since this is not cohesive with the previous datasets, we should change it to make merging dataframes easier later on. We use the openintro library to do the simple conversion.
```{r}
library(openintro)
brady_totals$state <-tolower(abbr2state(brady_totals$state))
head(brady_totals)
```

## 4. The Counted Fatal Shootings
First we simply need to concatenate the two years of data.
```{r}
counted1516 <- rbind(counted15, counted16)
head(counted1516)
```
Again, this data frame's list of states is in two-letter state abbreviations so let's change that.
```{r}
#library(openintro)
counted1516$state <-tolower(abbr2state(counted1516$state))
```
In the study, researchers "calculated descriptive statistics for the proportion of victims that were male, armed, and non-White." Thus we repeat here easily with dplyr. "Tally" is used to count the number of observations in each group.

They also calculated the annualized rate per 1,000,000.
```{r}
counted_stats <- counted1516 %>%
  group_by(state) %>%
  filter(classification == "Gunshot") %>%
  tally()

counted_stats$gunshot_filtered <- counted1516 %>%
  group_by(state) %>%
  filter(classification == "Gunshot",raceethnicity != "white", armed != "No", gender == "Male") %>%
  tally() %>%
  pull(n)

colnames(counted_stats)[2] <- "gunshot_tally"
counted_stats$gunshot_rate <- (counted_stats$gunshot_tally/census_stats$total_pop)*1000000
head(counted_stats)

```
## 5. Unemployment Data
This data is available online from the BLS, but there is no easy download of the table. It is also difficult to simply copy and paste; it doesn't hold it's table format. Thus, let's set up a web scraper to get the data.

To do this, we must use the *rvest* package. To view the HTML of a webpage, right-click and select "View page source."

Then we need to get the values from each column of the data table. *html_nodes* acts as a CSS selector. The sub0 class returns the state names, and the datavalue class corresponds to the values of both columns, Unemployment Rank and Rate. Since there is no differentiation of the two columns, and our "datavalues" alternate, we select subsets of this column to be the two seperate columns "rank" and "rate."
```{r}
library(rvest)
url <- read_html("https://www.bls.gov/lau/lastrk15.htm")
datavalues <- url%>%
  html_nodes('.datavalue') %>%
  html_text() %>%
  as.numeric()

state <- url%>%
  html_nodes('.sub0') %>%
  html_text()

unemployment<-as.data.frame(state[c(2:52)]) 
rate_select <- seq(3,103,by=2)
rank_select <- seq(4,104,by=2)
unemployment$unemployment_rate <- datavalues[rate_select]
unemployment$unemployment_rank <- datavalues[rank_select]
colnames(unemployment)[1] <- "state"

unemployment <- arrange(unemployment, state)
head(unemployment)
```
## 6. Population Density 2015
To manually find population density, we can calculate total 2015 population from the Census data and import land area to calculate population density. This is caculated because "good"" data for state population in 2015 was not available in a downloadable format and/or was not easy to scrape.

I select LND110210D because I looked at the data table at https://www.census.gov/geo/reference/state-area.html and compared values to find the correct column. This one corresponds to land area in square miles.
```{r}
library(dplyr)
totalPop <- census %>%
  filter(ORIGIN == 0, SEX == 0 ) %>%
  group_by(NAME) %>%
  summarize(total = sum(POPESTIMATE2015))

landSqMi <- land %>%
  select(Areaname,LND110210D)

head(landSqMi)
```
Since *landSqMi* gives us area for each town, merge on the state names to obtain only the area for each state.

Convert the state names to be all lowercase since the row values must match to merge successfully.
```{r}
names(landSqMi) <- c("NAME", "land_area")

landSqMi$NAME <- tolower(landSqMi$NAME)
totalPop$NAME <- tolower(totalPop$NAME)

popdensity <- merge(totalPop,landSqMi,by="NAME")
popdensity$density <- popdensity$total/popdensity$land_area

popdensity$total <- NULL
popdensity$land_area <-NULL

head(popdensity)
```
# Data analysis
## Poisson regression
### 1. No adjustment

$$\mu = t\exp{\beta_{0}}\exp{\beta_{1}X}$$

$$\frac{\mu}{t} = \exp{\beta_{0}}\exp{\beta_{1}X}$$
```{r}
require(ggplot2)
library(ggrepel)
require(sandwich)
require(msm)

census_stats$NAME <- tolower(census_stats$NAME)
p_df <- merge(brady_totals,counted_stats,by="state")
p_df <- merge(p_df,census_stats,by.x="state", by.y = "NAME")

summary(m1 <- glm(gunshot_tally ~  brady_scores + 
                  offset(log(total_pop)), 
                  family="poisson", data = p_df))

df1 <- data.frame("resultY" = 
                  exp(m1$coefficients[1])*exp(m1$coefficients[2]*p_df$brady_scores)*1000000/2,
                  "brady" = p_df$brady_scores,
                  "rates" = p_df$gunshot_tally/p_df$total_pop*1000000/2,
                  "abr" = state2abbr(p_df$state),
                  stringsAsFactors=FALSE) 

plt<- ggplot(df1, aes(x = brady)) + 
  geom_point(aes(y = rates, color = "data")) + 
  geom_line(aes(y = resultY , color = "fitted")) + 
  ggtitle("Fatal Police Shootings Poisson Regression") + 
  ylab("Annualized Rate of Fatal Police Shootings per 1 000 000") + 
  xlab("Firearm Legislative Strength Score") + 
  theme_bw() + theme(plot.title = element_text(hjust = 0.5 ))

plt
(plt <- plt +  geom_text_repel(aes(y = rates,label=abr)))

```
```{r}
str(df1)
```

Standard robust errors for parameter estimates
```{r}
cov.m1 <- vcovHC(m1, type="HC0")
std.err <- sqrt(diag(cov.m1))
r.est <- cbind(Estimate= coef(m1), "Robust SE" = std.err,
"Pr(>|z|)" = 2 * pnorm(abs(coef(m1)/std.err), lower.tail=FALSE),
LL = coef(m1) - 1.96 * std.err,
UL = coef(m1) + 1.96 * std.err)
r.est
```
### 2. Sociodemographic adjustment
We now construct a multivariate model to account "for state-level sociodemographic characteristics."

$$\mu = t\exp{\beta_{0}}\exp{\beta_{1}X_{1}}...\exp{\beta_{6}X_{6}}$$
```{r}
# mv for multivariate
summary(m2 <- glm(gunshot_tally ~  brady_scores + white + black + hispanic + male + age + offset(log(total_pop)), family="poisson", data = p_df))

m2_Xvars <- p_df %>%
  select(brady_scores,white,black,hispanic,male,age)
m2_Xvars$coef <- 1
m2_Xvars <- m2_Xvars[,c(7,1,2,3,4,5,6)]
X<- t(t(m2_Xvars)*(m2$coefficients))
# multiple e^col for each column in X

test <- rowSums(X[,2:7])
test <- exp(test)*X[,1]

df2 <- as.data.frame(test)
df2$X <- p_df$brady_scores
df2$trueY <- p_df$gunshot_tally

#ggplot(dat, aes(x = X)) + geom_point(aes(y = trueY, color = "data")) + geom_line(aes(y = test , color = "fitted")) + ggtitle("Fatal Police Shootings Poisson Regression") + ylab("Rate Per 1M Residents") + xlab("Brady Scores")
```
## Absolute Rate Differences


http://sphweb.bumc.bu.edu/otlt/MPH-Modules/QuantCore/PH717_ComparingFrequencies/PH717_ComparingFrequencies9.html
```{r}
library(kableExtra)
df1$count <- p_df$gunshot_tally
df1<-df1[order(df1$brady),]

table2<-data.frame("brady" = c(1:4), 
                   "mean_rates" = c(mean(df1$rates[1:12]),
                                    mean(df1$rates[13:25]),
                                    mean(df1$rates[26:38]),
                                    mean(df1$rates[39:50])))

table2$count_quartiles <- c(mean(df1$count[1:12]),
                                    mean(df1$count[13:25]),
                                    mean(df1$count[26:38]),
                                    mean(df1$count[39:50]))

table2$rate_diff <- abs(table2$mean_rates - table2$mean_rates[1])
table2$rate_ratio <- table2$mean_rates/table2$mean_rates[1]

table2$CI.L <-0
table2$CI.U <-0
for (i in c(2:4))
{
  std<-(1/table2$count_quartiles[1] + 1/table2$count_quartiles[i])^(1/2)
  table2$CI.U[i] <-exp(log(table2$rate_ratio[i]) + 1.96*std)
  table2$CI.L[i] <-exp(log(table2$rate_ratio[i]) - 1.96*std)
}

table2$count_quartiles<-NULL
names <- c("Firearm Legislative Strength Quartile",
           "Absolute Rate (SD)",
           "Absolute Rate Difference",
           "Incidence Rate Ratio",
           "CI: Lower",
           "CI: Upper")

table2 %>%
  kable(align = 'l',
      digits = 3, 
      col.names = names, 
      caption ="Change in Overall Fatal Police Shootings by State-Level Firearm Legislative Strength Quartile: United States, January 1, 2015â€“December 31, 2016") %>% 
  kable_styling(full_width = F) %>%
  add_header_above(c(" " = 3, "Unadjusted" = 3))
```


# Summary of results


